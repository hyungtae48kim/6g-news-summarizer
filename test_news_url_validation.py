#!/usr/bin/env python3
"""
ë‰´ìŠ¤ URL ê²€ì¦ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
- Google Newsì™€ The Vergeì—ì„œ ì‹¤ì œ ë‰´ìŠ¤ URL ì¶”ì¶œ
- URLì´ í™ˆí˜ì´ì§€ê°€ ì•„ë‹Œ ì‹¤ì œ ê¸°ì‚¬ë¡œ ì—°ê²°ë˜ëŠ”ì§€ í™•ì¸
- ë¬¸ì œê°€ ìˆëŠ” URL ì§„ë‹¨ ë° ë³´ê³ 
"""

import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urlparse

def validate_and_clean_url(url):
    """URL ìœ íš¨ì„± ê²€ì¦ ë° ì •ì œ (ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸ì™€ ë™ì¼)"""
    if not url:
        return ''

    url = url.strip()

    try:
        if not url.startswith(('http://', 'https://')):
            return ''

        url_pattern = re.compile(
            r'^https?://'
            r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)+[A-Z]{2,6}\.?|'
            r'localhost|'
            r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})'
            r'(?::\d+)?'
            r'(?:/?|[/?]\S+)$', re.IGNORECASE)

        if not url_pattern.match(url):
            return ''

        return url

    except Exception as e:
        print(f"âš ï¸ URL ê²€ì¦ ì˜¤ë¥˜: {str(e)[:50]}")
        return ''

def is_homepage_url(url):
    """URLì´ í™ˆí˜ì´ì§€ì¸ì§€ í™•ì¸ (ê²½ë¡œê°€ ì—†ê±°ë‚˜ ë£¨íŠ¸ë§Œ ìˆëŠ” ê²½ìš°)"""
    try:
        parsed = urlparse(url)
        path = parsed.path.strip('/')

        # ê²½ë¡œê°€ ì—†ê±°ë‚˜ ë¹ˆ ê²½ìš°
        if not path:
            return True, "ë£¨íŠ¸ ê²½ë¡œ (í™ˆí˜ì´ì§€)"

        # ê²½ë¡œê°€ ë§¤ìš° ì§§ì€ ê²½ìš° (í™ˆí˜ì´ì§€ ê°€ëŠ¥ì„± ë†’ìŒ)
        if len(path) < 5:
            return True, f"ê²½ë¡œê°€ ë„ˆë¬´ ì§§ìŒ: '{path}'"

        # ì¼ë°˜ì ì¸ í™ˆí˜ì´ì§€ ê²½ë¡œ
        homepage_paths = ['index.html', 'index.php', 'home', 'main']
        if path.lower() in homepage_paths:
            return True, f"í™ˆí˜ì´ì§€ ê²½ë¡œ: '{path}'"

        return False, f"ê¸°ì‚¬ ê²½ë¡œ: '{path}'"

    except Exception as e:
        return False, f"íŒŒì‹± ì˜¤ë¥˜: {str(e)}"

def test_google_news_urls(query="6G wireless", num_results=5):
    """Google News RSSì—ì„œ URL ì¶”ì¶œ ë° ê²€ì¦"""

    print("\n" + "="*70)
    print("ğŸ§ª Google News URL í…ŒìŠ¤íŠ¸")
    print("="*70)
    print(f"ê²€ìƒ‰ì–´: {query}")
    print(f"ê²°ê³¼ ìˆ˜: {num_results}")
    print()

    url = f"https://news.google.com/rss/search?q={query}&hl=ko&gl=KR&ceid=KR:ko"

    try:
        response = requests.get(url, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'xml')
        items = soup.find_all('item', limit=num_results)

        results = []
        for i, item in enumerate(items, 1):
            title = item.title.text if item.title else 'No title'

            # Method 1: Extract from <source> tag
            source_tag = item.find('source')
            source_url = source_tag.get('url') if source_tag else None

            # Method 2: Extract from <link> tag (Google redirect)
            link_url = item.link.text if item.link else None

            # Validate both URLs
            validated_source = validate_and_clean_url(source_url) if source_url else ''
            validated_link = validate_and_clean_url(link_url) if link_url else ''

            # Check if homepage
            is_homepage_source, source_reason = is_homepage_url(validated_source) if validated_source else (None, 'No URL')
            is_homepage_link, link_reason = is_homepage_url(validated_link) if validated_link else (None, 'No URL')

            result = {
                'index': i,
                'title': title[:80] + '...' if len(title) > 80 else title,
                'source_url': validated_source,
                'link_url': validated_link,
                'is_homepage_source': is_homepage_source,
                'source_reason': source_reason,
                'is_homepage_link': is_homepage_link,
                'link_reason': link_reason
            }
            results.append(result)

            # ì¶œë ¥
            print(f"{i}. {result['title']}")
            print(f"   Source URL: {validated_source or 'âŒ ì—†ìŒ'}")
            if validated_source:
                status = "âš ï¸ í™ˆí˜ì´ì§€" if is_homepage_source else "âœ… ê¸°ì‚¬"
                print(f"   â””â”€ {status}: {source_reason}")

            print(f"   Link URL: {validated_link or 'âŒ ì—†ìŒ'}")
            if validated_link:
                status = "âš ï¸ ë¦¬ë‹¤ì´ë ‰íŠ¸" if 'news.google.com' in validated_link else "âœ… ì§ì ‘ ë§í¬"
                print(f"   â””â”€ {status}")
            print()

        # í†µê³„
        total = len(results)
        homepage_count = sum(1 for r in results if r['is_homepage_source'])
        valid_count = total - homepage_count

        print("ğŸ“Š í†µê³„:")
        print(f"   ì „ì²´: {total}ê°œ")
        print(f"   âœ… ìœ íš¨í•œ ê¸°ì‚¬ URL: {valid_count}ê°œ")
        print(f"   âš ï¸ í™ˆí˜ì´ì§€ URL: {homepage_count}ê°œ")

        return results

    except Exception as e:
        print(f"âŒ Google News í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return []

def test_verge_urls(query="6G wireless", num_results=5):
    """The Verge Atom feedì—ì„œ URL ì¶”ì¶œ ë° ê²€ì¦"""

    print("\n" + "="*70)
    print("ğŸ§ª The Verge URL í…ŒìŠ¤íŠ¸")
    print("="*70)
    print(f"í•„í„°ë§: {query}")
    print(f"ê²°ê³¼ ìˆ˜: {num_results}")
    print()

    url = "https://www.theverge.com/rss/index.xml"

    try:
        response = requests.get(url, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'xml')
        entries = soup.find_all('entry')

        results = []
        query_lower = query.lower()

        for entry in entries:
            if len(results) >= num_results:
                break

            title = entry.find('title').text if entry.find('title') else ''
            summary_elem = entry.find('summary')
            content_elem = entry.find('content')

            description = ''
            if summary_elem:
                description = summary_elem.text
            elif content_elem:
                description = BeautifulSoup(content_elem.text, 'html.parser').get_text()

            # Filter by query
            text_to_search = (title + ' ' + description).lower()
            query_words = query_lower.split()

            if not any(word in text_to_search for word in query_words):
                continue

            # Extract URL
            link_elem = entry.find('link', {'rel': 'alternate'})
            if not link_elem:
                link_elem = entry.find('link')

            extracted_url = link_elem.get('href') if link_elem else ''
            validated_url = validate_and_clean_url(extracted_url)

            # Check if homepage
            is_homepage, reason = is_homepage_url(validated_url) if validated_url else (None, 'No URL')

            result = {
                'index': len(results) + 1,
                'title': title[:80] + '...' if len(title) > 80 else title,
                'url': validated_url,
                'is_homepage': is_homepage,
                'reason': reason
            }
            results.append(result)

            # ì¶œë ¥
            print(f"{result['index']}. {result['title']}")
            print(f"   URL: {validated_url or 'âŒ ì—†ìŒ'}")
            if validated_url:
                status = "âš ï¸ í™ˆí˜ì´ì§€" if is_homepage else "âœ… ê¸°ì‚¬"
                print(f"   â””â”€ {status}: {reason}")
            print()

        # í†µê³„
        total = len(results)
        homepage_count = sum(1 for r in results if r['is_homepage'])
        valid_count = total - homepage_count

        print("ğŸ“Š í†µê³„:")
        print(f"   ì „ì²´: {total}ê°œ")
        print(f"   âœ… ìœ íš¨í•œ ê¸°ì‚¬ URL: {valid_count}ê°œ")
        print(f"   âš ï¸ í™ˆí˜ì´ì§€ URL: {homepage_count}ê°œ")

        return results

    except Exception as e:
        print(f"âŒ The Verge í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return []

def test_url_accessibility(url, timeout=5):
    """URLì´ ì‹¤ì œë¡œ ì ‘ê·¼ ê°€ëŠ¥í•œì§€ í…ŒìŠ¤íŠ¸ (HTTP HEAD ìš”ì²­)"""
    try:
        response = requests.head(url, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }, timeout=timeout, allow_redirects=True)

        # ìµœì¢… URL (ë¦¬ë‹¤ì´ë ‰íŠ¸ í›„)
        final_url = response.url

        # ìƒíƒœ ì½”ë“œ
        status_code = response.status_code

        # ì„±ê³µ ì—¬ë¶€
        is_accessible = (200 <= status_code < 400)

        return {
            'accessible': is_accessible,
            'status_code': status_code,
            'final_url': final_url,
            'redirected': (final_url != url)
        }

    except Exception as e:
        return {
            'accessible': False,
            'status_code': None,
            'final_url': None,
            'redirected': False,
            'error': str(e)
        }

def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""

    print("\n" + "="*70)
    print("ğŸ” ë‰´ìŠ¤ URL ê²€ì¦ í…ŒìŠ¤íŠ¸")
    print("="*70)
    print("ëª©ì : ë‰´ìŠ¤ URLì´ í™ˆí˜ì´ì§€ê°€ ì•„ë‹Œ ì‹¤ì œ ê¸°ì‚¬ë¡œ ì—°ê²°ë˜ëŠ”ì§€ í™•ì¸")
    print("="*70)

    # Google News í…ŒìŠ¤íŠ¸
    google_results = test_google_news_urls(query="6G wireless communication", num_results=5)

    # The Verge í…ŒìŠ¤íŠ¸
    verge_results = test_verge_urls(query="wireless", num_results=5)

    # ì¢…í•© ê²°ê³¼
    print("\n" + "="*70)
    print("ğŸ“‹ ì¢…í•© ê²°ê³¼")
    print("="*70)

    # Google News ë¬¸ì œì 
    google_homepage_count = sum(1 for r in google_results if r['is_homepage_source'])
    if google_homepage_count > 0:
        print(f"\nâš ï¸ Google News ë¬¸ì œì :")
        print(f"   {google_homepage_count}/{len(google_results)}ê°œ URLì´ í™ˆí˜ì´ì§€ë¡œ ì—°ê²°ë¨")
        print(f"   í•´ê²° ë°©ë²•: <source url='...'> íƒœê·¸ ëŒ€ì‹  ë‹¤ë¥¸ ë°©ë²• ì‹œë„ í•„ìš”")
    else:
        print(f"\nâœ… Google News: {len(google_results)}ê°œ URL ëª¨ë‘ ì •ìƒ")

    # The Verge ë¬¸ì œì 
    verge_homepage_count = sum(1 for r in verge_results if r['is_homepage'])
    if verge_homepage_count > 0:
        print(f"\nâš ï¸ The Verge ë¬¸ì œì :")
        print(f"   {verge_homepage_count}/{len(verge_results)}ê°œ URLì´ í™ˆí˜ì´ì§€ë¡œ ì—°ê²°ë¨")
        print(f"   í•´ê²° ë°©ë²•: <link> íƒœê·¸ ì¶”ì¶œ ë°©ì‹ ì¬ê²€í†  í•„ìš”")
    else:
        print(f"\nâœ… The Verge: {len(verge_results)}ê°œ URL ëª¨ë‘ ì •ìƒ")

    # ì¶”ê°€ í…ŒìŠ¤íŠ¸: ì‹¤ì œ ì ‘ê·¼ì„± í™•ì¸ (ìƒ˜í”Œ 3ê°œ)
    print("\n" + "="*70)
    print("ğŸŒ URL ì ‘ê·¼ì„± í…ŒìŠ¤íŠ¸ (ìƒ˜í”Œ)")
    print("="*70)

    sample_urls = []
    if google_results:
        sample_urls.extend([r['source_url'] for r in google_results[:2] if r['source_url']])
    if verge_results:
        sample_urls.extend([r['url'] for r in verge_results[:2] if r['url']])

    for i, url in enumerate(sample_urls[:3], 1):
        print(f"\n{i}. í…ŒìŠ¤íŠ¸ ì¤‘: {url[:60]}...")
        access_info = test_url_accessibility(url, timeout=5)

        if access_info['accessible']:
            print(f"   âœ… ì ‘ê·¼ ê°€ëŠ¥ (HTTP {access_info['status_code']})")
            if access_info['redirected']:
                print(f"   ğŸ”„ ë¦¬ë‹¤ì´ë ‰íŠ¸ë¨:")
                print(f"      â†’ {access_info['final_url'][:70]}...")
        else:
            print(f"   âŒ ì ‘ê·¼ ì‹¤íŒ¨")
            if access_info.get('error'):
                print(f"      ì˜¤ë¥˜: {access_info['error']}")

    print("\n" + "="*70)
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print("="*70)

if __name__ == "__main__":
    main()
